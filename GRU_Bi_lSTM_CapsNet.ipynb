{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGvpNIxi2FsBuUfzZDCyRy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33TCotujtM8e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    InputLayer,\n",
        "    LSTM,\n",
        "    Bidirectional,\n",
        "    Embedding,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Convolution1D,\n",
        "    MaxPooling1D,\n",
        "    BatchNormalization,\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "BtFGA-o0tREP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def create_dataset(data_path: str) -> Tuple[List[str], List[int]]:\n",
        "    dataset = pd.read_csv(data_path)\n",
        "    dataset = dataset.sample(frac=1).reset_index(drop=True)  # shuffle the dataset\n",
        "    return list(dataset[\"sequence\"]), list(dataset[\"label\"])\n",
        "\n",
        "\n",
        "def split_dataset(\n",
        "    sequences_list: List[str], labels_list: List[int], train_size: float = 0.8\n",
        ") -> Tuple[List[str], List[str], List[str], List[int], List[int], List[int]]:\n",
        "    dataset = pd.DataFrame({\"sequence\": sequences_list, \"label\": labels_list})\n",
        "    dataset = dataset.sample(frac=1, random_state=1)\n",
        "    train, remaining = train_test_split(dataset, train_size=train_size, random_state=2)\n",
        "    valid, test = train_test_split(remaining, test_size=0.5, random_state=3)\n",
        "    x_train, x_valid, x_test = train[\"sequence\"], valid[\"sequence\"], test[\"sequence\"]\n",
        "    y_train, y_valid, y_test = train[\"label\"], valid[\"label\"], test[\"label\"]\n",
        "    return (\n",
        "        list(x_train),\n",
        "        list(x_valid),\n",
        "        list(x_test),\n",
        "        list(y_train),\n",
        "        list(y_valid),\n",
        "        list(y_test),\n",
        "    )\n",
        "\n",
        "\n",
        "def one_hot_encoding(\n",
        "    sequence: str,\n",
        "    max_seq_length: int = 1024,\n",
        "    CONSIDERED_AA: str = \"ACDEFGHIKLMNPQRSTVWY\",\n",
        "):\n",
        "    # adapt sequence size\n",
        "    if len(sequence) > max_seq_length:\n",
        "        # short the sequence\n",
        "        sequence = sequence[:max_seq_length]\n",
        "    else:\n",
        "        # pad the sequence\n",
        "        sequence = sequence + \".\" * (max_seq_length - len(sequence))\n",
        "\n",
        "    # encode sequence\n",
        "    encoded_sequence = np.zeros((max_seq_length, len(CONSIDERED_AA)))  # (1000, 20)\n",
        "    for i, amino_acid in enumerate(sequence):\n",
        "        if amino_acid in CONSIDERED_AA:\n",
        "            encoded_sequence[i][CONSIDERED_AA.index(amino_acid)] = 1\n",
        "    model_input = np.expand_dims(encoded_sequence, 0)  # add batch dimension\n",
        "\n",
        "    return model_input  # (1, 1000, 20)\n",
        "\n",
        "\n",
        "def preprocess_word_embedding_encoding(\n",
        "    sequence: str,\n",
        "    max_seq_length: int = 1024,\n",
        "    CONSIDERED_AA: str = \"ACDEFGHIKLMNPQRSTVWY\",\n",
        "):\n",
        "    # amino acids encoding\n",
        "    aa_mapping = {aa: i + 1 for i, aa in enumerate(CONSIDERED_AA)}\n",
        "\n",
        "    # adapt sequence size\n",
        "    if len(sequence) > max_seq_length:\n",
        "        # short the sequence\n",
        "        sequence = sequence[:max_seq_length]\n",
        "    else:\n",
        "        # pad the sequence\n",
        "        sequence = sequence + \".\" * (max_seq_length - len(sequence))\n",
        "\n",
        "    # encode sequence\n",
        "    encoded_sequence = np.zeros((max_seq_length,))  # (1000,)\n",
        "    for i, amino_acid in enumerate(sequence):\n",
        "        if amino_acid in CONSIDERED_AA:\n",
        "            encoded_sequence[i] = aa_mapping[amino_acid]\n",
        "    model_input = np.expand_dims(encoded_sequence, 0)  # add batch dimension\n",
        "\n",
        "    return model_input  # (1, 1000)"
      ],
      "metadata": {
        "id": "rZ56hfwItR3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_keras\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_keras\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return tn / (tn + fp + K.epsilon())\n",
        "\n",
        "\n",
        "def negative_predictive_value(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "    return tn / (tn + fn + K.epsilon())\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "    y_pred = K.clip(y_pred, 0, 1)\n",
        "\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    num = (1 + beta ** 2) * (p * r)\n",
        "    den = (beta ** 2 * p + r + K.epsilon())\n",
        "    return K.mean(num / den)\n",
        "\n",
        "\n",
        "def matthews_correlation_coefficient(y_true, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "\n",
        "    num = tp * tn - fp * fn\n",
        "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "    return num / K.sqrt(den + K.epsilon())\n",
        "\n",
        "\n",
        "def equal_error_rate(y_true, y_pred):\n",
        "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "\n",
        "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
        "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
        "\n",
        "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
        "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
        "    body = lambda t, fpr, fnr: (\n",
        "        t + 0.001,\n",
        "        tf.divide(tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
        "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
        "    )\n",
        "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
        "    eer = (fpr + fnr) / 2\n",
        "\n",
        "    return eer"
      ],
      "metadata": {
        "id": "gxB5ffy3tXE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB1063.csv')\n",
        "test = pd.read_csv('/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB186.csv')\n",
        "\n",
        "x_train_val = train['sequence']\n",
        "y_train_val = train['label']\n",
        "x_test = test['sequence']"
      ],
      "metadata": {
        "id": "896gsIPTtZp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_pdb186():\n",
        "    SEED = 42\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    # the data, shuffled and split between train and test sets\n",
        "    # TRAIN_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB14189.csv\"\n",
        "    # TEST_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB2272.csv\"\n",
        "\n",
        "    TRAIN_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB1063.csv\"\n",
        "    TEST_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB186.csv\"\n",
        "\n",
        "    CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "    # embedding and convolution parameters\n",
        "    MAX_SEQ_LENGTH = 1024\n",
        "    VOCAB_SIZE = len(CONSIDERED_AA)\n",
        "\n",
        "\n",
        "    # create train dataset\n",
        "    sequences_train, labels_train = create_dataset(data_path=TRAIN_SET)\n",
        "\n",
        "    # create test dataset\n",
        "    sequences_test, labels_test = create_dataset(data_path=TEST_SET)\n",
        "\n",
        "    # encode sequences\n",
        "    sequences_train_encoded = np.concatenate(\n",
        "        [\n",
        "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
        "            for seq in sequences_train\n",
        "        ],\n",
        "        axis=0,\n",
        "    )  # (14189, 1000, 20)\n",
        "    sequences_test_encoded = np.concatenate(\n",
        "        [\n",
        "            one_hot_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
        "            for seq in sequences_test\n",
        "        ],\n",
        "        axis=0,\n",
        "    )  # (2272, 1000, 20)\n",
        "\n",
        "    # encode labels\n",
        "    labels_train_encoded = to_categorical(\n",
        "        labels_train, num_classes=2, dtype=\"float32\"\n",
        "    )  # (14189, 2)\n",
        "    labels_test_encoded = to_categorical(\n",
        "        labels_test, num_classes=2, dtype=\"float32\"\n",
        "    )  # (2272, 2)\n",
        "\n",
        "    return sequences_train_encoded, labels_train_encoded, sequences_test_encoded, labels_test_encoded"
      ],
      "metadata": {
        "id": "ks0rOtKstZm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_embedding_word2vec():\n",
        "    SEED = 42\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "    # set amino acids to consider\n",
        "    CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "    # embedding and convolution parameters\n",
        "    MAX_SEQ_LENGTH =1024\n",
        "    VOCAB_SIZE = len(CONSIDERED_AA)\n",
        "    # POOL_LENGTH = 3\n",
        "    EMBEDDING_SIZE = 128\n",
        "\n",
        "    # training parameters\n",
        "    # BATCH_SIZE = 128\n",
        "    # NUM_EPOCHS = 800\n",
        "    SAVED_MODEL_PATH = (\n",
        "        \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/logs/model_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".hdf5\"\n",
        "    )\n",
        "    # TRAIN_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB14189.csv\"\n",
        "    # TEST_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB2272.csv\"\n",
        "\n",
        "    TRAIN_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB1063.csv\"\n",
        "    TEST_SET = \"/gdrive/MyDrive/DNA Binding Protein/Dbp app/data/PDB186.csv\"\n",
        "\n",
        "    # create train dataset\n",
        "    sequences_train, labels_train = create_dataset(data_path=TRAIN_SET)\n",
        "\n",
        "    # create test dataset\n",
        "    sequences_test, labels_test = create_dataset(data_path=TEST_SET)\n",
        "\n",
        "    # encode sequences\n",
        "    sequences_train_encoded = np.concatenate([preprocess_word_embedding_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
        "    for seq in sequences_train\n",
        "    ],\n",
        "    axis=0,\n",
        "    )  # (14189, 800)\n",
        "\n",
        "    sequences_test_encoded = np.concatenate([\n",
        "    preprocess_word_embedding_encoding(seq, MAX_SEQ_LENGTH, CONSIDERED_AA)\n",
        "    for seq in sequences_test\n",
        "    ],\n",
        "    axis=0,\n",
        "    )  # (2272, 800)\n",
        "\n",
        "    train_word2vec = []\n",
        "    test_word2vec = []\n",
        "    train_sequences = sequences_train_encoded.astype(int)\n",
        "    test_sequences = sequences_test_encoded.astype(int)\n",
        "    # all_sequences = sequences_train_encoded1\n",
        "\n",
        "    embeddings = np.load('/gdrive/MyDrive/DNA Binding Protein/Dbp app/embeddings.npy')\n",
        "    lookup_matrix = embeddings\n",
        "\n",
        "    for idx in train_sequences:\n",
        "        train_word2vec.append(lookup_matrix[idx])\n",
        "\n",
        "    train_word2vec =  np.stack(train_word2vec)\n",
        "\n",
        "    for idx in test_sequences:\n",
        "        test_word2vec.append(lookup_matrix[idx])\n",
        "\n",
        "    test_word2vec =  np.stack(test_word2vec)\n",
        "\n",
        "\n",
        "\n",
        "    # encode labels\n",
        "    labels_train_encoded = to_categorical(\n",
        "    labels_train, num_classes=2, dtype=\"float32\"\n",
        "    )  # (14189, 2)\n",
        "    labels_test_encoded = to_categorical(\n",
        "    labels_test, num_classes=2, dtype=\"float32\"\n",
        "    )  # (2272, 2)\n",
        "\n",
        "    return train_word2vec, labels_train_encoded, test_word2vec, labels_test_encoded"
      ],
      "metadata": {
        "id": "uryJW13_tXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
        "    return scale * x\n",
        "\n",
        "\n",
        "#define our own softmax function instead of K.softmax\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex/K.sum(ex, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "#A Capsule Implement with Pure Keras\n",
        "class Capsule(Layer):\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3, share_weights=True, activation='squash', **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Capsule, self).build(input_shape)\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(1, input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "        else:\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.W = self.add_weight(name='capsule_kernel',\n",
        "                                     shape=(input_num_capsule,\n",
        "                                            input_dim_capsule,\n",
        "                                            self.num_capsule * self.dim_capsule),\n",
        "                                     initializer='glorot_uniform',\n",
        "                                     trainable=True)\n",
        "\n",
        "    def call(self, u_vecs):\n",
        "        if self.share_weights:\n",
        "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
        "        else:\n",
        "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(u_vecs)[0]\n",
        "        input_num_capsule = K.shape(u_vecs)[1]\n",
        "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
        "                                            self.num_capsule, self.dim_capsule))\n",
        "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
        "        #final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "\n",
        "        b = K.zeros_like(u_hat_vecs[:,:,:,0]) #shape = [None, num_capsule, input_num_capsule]\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = K.batch_dot(c, u_hat_vecs, [2, 2])\n",
        "            if K.backend() == 'theano':\n",
        "                o = K.sum(o, axis=1)\n",
        "            if i < self.routings - 1:\n",
        "                o = K.l2_normalize(o, -1)\n",
        "                b = K.batch_dot(o, u_hat_vecs, [2, 3])\n",
        "                if K.backend() == 'theano':\n",
        "                    b = K.sum(b, axis=1)\n",
        "\n",
        "        return self.activation(o)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n"
      ],
      "metadata": {
        "id": "2YDbDn5ltW_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset,\n",
        "not just on MNIST.\n",
        "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
        "uncommenting them and commenting their counterparts.\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras import initializers, layers\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional\n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=tf.shape(x)[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_vector]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_vector]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_capsule = dim_vector = 1.\n",
        "\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
        "    :param num_routing: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_vector = dim_vector\n",
        "        self.num_routing = num_routing\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=(self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_vector, self.input_dim_capsule),\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        inputs_expand = K.expand_dims(inputs, 1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_vector, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_vector, input_dim_capsule]^T -> [dim_vector].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_vector]\n",
        "        inputs_hat = K.map_fn(lambda x: own_batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        \"\"\"\n",
        "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        b = K.zeros(shape=[self.batch_size, self.num_capsule, self.input_num_capsule])\n",
        "        def body(i, b, outputs):\n",
        "            c = tf.nn.softmax(b, dim=1)  # dim=2 is the num_capsule dimension\n",
        "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))\n",
        "            if i != 1:\n",
        "                b = b + K.batch_dot(outputs, inputs_hat, [2, 3])\n",
        "            return [i-1, b, outputs]\n",
        "        cond = lambda i, b, inputs_hat: i > 0\n",
        "        loop_vars = [K.constant(self.num_routing), b, K.sum(inputs_hat, 2, keepdims=False)]\n",
        "        shape_invariants = [tf.TensorShape([]),\n",
        "                            tf.TensorShape([None, self.num_capsule, self.input_num_capsule]),\n",
        "                            tf.TensorShape([None, self.num_capsule, self.dim_vector])]\n",
        "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
        "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
        "        \"\"\"\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # In forward pass, `inputs_hat_stopped` = `inputs_hat`;\n",
        "        # In backward, no gradient can flow from `inputs_hat_stopped` back to `inputs_hat`.\n",
        "        inputs_hat_stopped = K.stop_gradient(inputs_hat)\n",
        "\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=(K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule))\n",
        "\n",
        "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
        "        for i in range(self.num_routing):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # At last iteration, use `inputs_hat` to compute `outputs` in order to backpropagate gradient\n",
        "            if i == self.num_routing - 1:\n",
        "                # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_vector]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [input_num_capsule] x [input_num_capsule, dim_vector] -> [dim_vector].\n",
        "                # outputs.shape=[None, num_capsule, dim_vector]\n",
        "                outputs = squash(own_batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
        "            else:  # Otherwise, use `inputs_hat_stopped` to update `b`. No gradients flow on this path.\n",
        "                outputs = squash(own_batch_dot(c, inputs_hat_stopped, [2, 2]))\n",
        "\n",
        "                # outputs.shape =  [None, num_capsule, dim_vector]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_vector]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_vector] x [input_num_capsule, dim_vector]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                b += own_batch_dot(outputs, inputs_hat_stopped, [2, 3])\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_vector])\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding, name):\n",
        "    \"\"\"\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_vector: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
        "    \"\"\"\n",
        "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding, name=name)(inputs)\n",
        "    # conv1Norm = layers.BatchNormalization(epsilon=0.001, axis=-1, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(output)\n",
        "    # outputs = layers.Reshape(target_shape=[-1, dim_vector])(conv1Norm)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
        "    outputs = layers.Dropout(0.6)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n"
      ],
      "metadata": {
        "id": "Zb8P4niFtW8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LeakyReLU, Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D, concatenate,LSTM,Conv1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "gru_len = 64\n",
        "Routings = 2\n",
        "Num_capsule = 10\n",
        "Dim_capsule = 16\n",
        "Dim_capsule1 = 32\n",
        "dropout_p = 0.25\n",
        "rate_drop_dense = 0.28\n",
        "\n",
        "max_features = 20000\n",
        "maxlen = 1000\n",
        "embed_size1 = 20\n",
        "embed_size2 = 128\n",
        "\n",
        "def get_model():\n",
        "    input1 = layers.Input(shape=(1024,20))\n",
        "    input2 = layers.Input(shape=(1024,128))\n",
        "\n",
        "    x = GRU(32, return_sequences=True)(input1)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    x1 = Bidirectional(LSTM(256, return_sequences=True))(input2)\n",
        "    x1 = Dropout(0.2)(x1)\n",
        "    x = concatenate([x,x1], axis=-1)\n",
        "    x = Conv1D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    primary_caps = PrimaryCap(x, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"primary_caps\")\n",
        "    category_caps = CapsuleLayer(num_capsule=2, dim_vector=8, num_routing=2, name='category_caps')(primary_caps)\n",
        "    out_caps = Length(name='out_caps')(category_caps)\n",
        "    model = Model(inputs=[input1,input2], outputs=out_caps)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "yb2rGeAatW56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NfO7MgmUtW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "epoch=30\n",
        "# unpacking the data\n",
        "# (x_train, y_train), (x_test, y_test) = data\n",
        "\n",
        "# set amino acids to consider\n",
        "CONSIDERED_AA = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "\n",
        "X_train_one_hot,y_train,X_test_one_hot,y_test = load_data_pdb186()\n",
        "X_train_word2vec,y_train,X_test_word2vec,y_test = load_data_embedding_word2vec()\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# reducing = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1)\n",
        "reducing=EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1)\n",
        "SAVED_MODEL_PATH = (\"/gdrive/MyDrive/DNA Binding Protein/Dbp app/logs/model_\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \".hdf5\")\n",
        "\n",
        "lr_decay = LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9** epoch))\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=tf.optimizers.RMSprop(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[\"accuracy\", specificity, \"Precision\", \"Recall\", \"AUC\", matthews_correlation_coefficient])\n",
        "\n",
        "\n",
        "history=model.fit([X_train_one_hot, X_train_word2vec], y_train, batch_size=128, epochs=epoch,\n",
        "                  # validation_split=0.01,\n",
        "                  callbacks=[lr_decay])\n",
        "\n",
        "\n",
        "scores = model.evaluate([X_test_one_hot, X_test_word2vec], y_test, verbose=1)\n",
        "# scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "print(f'Score : {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%; {model.metrics_names[2]} of {scores[2]}; {model.metrics_names[3]} of {scores[3]}; {model.metrics_names[4]} of {scores[4]}; {model.metrics_names[5]} of {scores[5]}; {model.metrics_names[6]} of {scores[6]}')\n"
      ],
      "metadata": {
        "id": "g_f_YflAtW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DnPMnB26tWxu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}